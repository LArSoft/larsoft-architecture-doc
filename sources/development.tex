% \section{Deployment view: development and extensibility}
% \label{sec:Development}

The extensibility of LArSoft is largely based on the underlying
framework, \ART. The \ART framework processes physics \emph{event}
independently, executing on each of them a sequence of \emph{modules}.
An event is defined by an input module.
In most Experiments it is bound to a single pulsed beam interaction with the detector,
but test beam Experiments, non-beam Experiments and non-beam analyses (\eg proton decay)
may need to define different event boundaries.
The framework also provides a list of global \emph{services} that modules can rely on.
Examples of services implemented by LArSoft include
the description of detector geometry and channel mapping,
the set of detector configuration parameters,
and access to TPC channel quality information.

In this section we describe the development environment
and then focus on the main handles LArSoft offers developers for the sake of extensibility,
including new serializable data structures, new algorithms
and the use of external libraries.


\subsection{Development environment}
\label{ssec:Development:Environment}

LArSoft is designed for and supports the use of a development environment based on:
\begin{itemize}
   \item UNIX Product Support (\UPS) for access to dependent packages
   \item \cetbuildtools\cite{cetbuildtools} as build system
   \item Multi-Repository Build\cite{MRB} (\MRB)  to coordinate build and execute software from different repositories
   \item \git (recommended) or \SVN for version control
\end{itemize}
The following description assumes the prerequisite availability of all these tools.

LArSoft is fully supported on the following platforms:
\begin{itemize}
   \item Scientific Linux Fermi: version 6
   \item Darwin: version 13 (OS X 10.9 ``Maverick'') and 14 (OS X 10.10 ``Yosemite'')
\end{itemize}
LArSoft typically supports the two most recent versions of these operating systems\footnote{
The actual supported versions depend also on the underlying support of the O.S. by Fermilab.
}.
Support is also planned for the long term support release of Ubuntu Linux (16.04 LTS).

A typical workflow starts with the set up of a working area.
After the area is created the first time, subsequent utilization of it requires just a simple set up.
LArSoft provides a script for this set up,
and it is common practice for the Experiments to provide customized ones.

The development, whether it is creation of new code or modification of existing one,
follows the following workflow:
\begin{enumerate}
   \item \emph{development-specific set up} of the existing working area
   \item importing the source code to be modified, if any; this code will persist in the area
   \item modifications as needed
   \item building
   \item optional (and recommended) execution of a standard test suite
   \item installation for running
\end{enumerate}

The execution of LArSoft code including user development, as described above,
follows this workflow:
\begin{enumerate}
   \item \emph{run-time specific set up} of the existing working area
   \item preparation of job configuration as needed
   \item execution of the software
\end{enumerate}
The execution of LArSoft code as distributed, without modification,
has a simpler set up that does not require a development working area.\\
LArSoft and the Experiments provide a vast selection of configurations ready to run, making the second step optional.
Development and execution set up can coexist in the same environment at the same time.

LArSoft currently provides no facility to execute code remotely,
including job submission to remote clusters.
The Experiments supply workflows and scripts for this type of execution.



\subsection{Testing}
\label{ssec:Development:testing}

LArSoft development model allows multiple contributors to modify the
code at the same time. This model can create conflicts and dysfunction
in the code. Tests are instrumental to the early detection of such
defects. LArSoft includes tests at two levels, called \emph{unit tests}
and \emph{integration tests}.

Unit tests exercise a limited part of the system, typically a single algorithm.
Ideally a unit test for an algorithm should test all the functions of that algorithm.
In practice, tests for complex algorithms tend to set up and test a few known typical cases.
Unit tests can be added at the same time the tested code is beging developed.
They are run in the development environment:
as such, they are the quickest mean to exercise newly written code.

Integration tests involve the framework and one or more processing modules.
These tests can reproduce real user scenarios,
for example a part of the official processing chain of an Experiment,
and they can compare new and historical results.
LArSoft tools allow these tests to be run on demand at any time,
and a standard suite of tests is automatically and periodically run
as part of LArSoft Continuous Integration system.


\subsection{Data products}
\label{ssec:Development:DataProducts}

LArSoft provides a basic set of data structures.
Each structure is associated to a simple concept and a set of related quantities.
For example, \texttt{raw::RawDigit} describes the raw data as read from a TPC channel;
\texttt{recob::Cluster} describes a set of correlated hits observed on a wire plane;
\texttt{anab::Calorimetry} contains information about calibrated energy of a track.
A \emph{data product} is data that can be serialized and then recovered for further processing.
A data product can be:
\begin{itemize}
   \item a data structure
   \item a collection of data structures
   \item a set of associations between data structures
\end{itemize}

LArSoft strongly recommends data structures designed for serialization to follow some standard prescriptions:
\begin{itemize}
   \item simple: they contain just data, and trivial logic to access it;
      more complex elaborations belong to algorithms;
   \item concrete: they can be instantiated;
   \item containing data members from a restricted selection:
      \begin{itemize}
         \item fundamental C++ types (note that pointers are not fundamental);
         \item other types suitable themselves as data products
            (including collections as described below).
      \end{itemize}
\end{itemize}
Limitations of the ROOT I/O system impose restrictions on the types of allowed data members,
\eg on the set of supported C++11 containers.

Collections of data structures also undergo some prescriptions:
\begin{itemize}
   \item when contained as member of other data products,
      standard containers (from C++ STL) are preferred,
      favouring fixed-size arrays and STL vectors;
   \item when passed to the framework to be saved directly,
      STL vectors are strongly recommended;
   \item contained types must themselves be suitable as data products
      (as described above).
\end{itemize}

Relations between data products are expressed by \emph{associations}.
Associations are data products provided by \ART
that can relate a data product, or an element within a collection of data products,
to another data product or element.
Examples of use in LArSoft include associations between a
reconstructed hit and the calibrated signal it's reconstructed from, and
between a cluster and all the hits that constitute it.

Data products have a fundamental structural role:
they act as messages to be exchanged between algorithms.
As such, they are also the format in which most of the algorithm results are saved.
This allows to arbitrary split the processing chain in multiple sequences of jobs.


\subsection{User code}
\label{ssec:Development:UserCode}

Algorithms constitute, together with data products, the heart of LArSoft,
and the ability for the users to add their own algorithm is central to its design.
In fact, LArSoft algorithms differ from users' algorithms
only in the consideration that their purpose is of general interest.
Indeed, most of the algorithms in LArSoft were written by users to solve their own specific problems,
and then adopted into the common toolkit.
LArSoft encourages users to produce algorithms that perform correctly on \emph{any} liquid argon detector,
and to integrate them into LArSoft itself.

\begin{figure}
   \centering
   \includegraphics{figures/LArSoftFactorizationModelAndTests}
   \caption[LArSoft algorithm and service model]{
      \label{fig:AlgorithmModel}
      LArSoft algorithm and service model.
      Black lines represent ownership.
      The coloured arrows show the path the algorithm obtains the provider through.
      The green line contours the standard execution environment.
      Dotted lines describe testing environments:
      both service providers and algorithms can be tested without involving the full framework.
   }
\end{figure}

The preferred model for algorithm design is represented in \cref{fig:AlgorithmModel}.
We refer to this as \emph{factorization} model.
The underlying principle it is that the algorithm must be independently
testable and portable, using the minimal set of necessary dependences.
This also allows for the algorithms to be used in contexts where the
\ART framework is not available,
provided that some other system supplies equivalent functionalities as,
and only when, needed. The model is made of two layers:
\begin{enumerate}
   \item
      the algorithm, in the form of a class that
      \begin{itemize}
         \item
            is configurable with FHiCL parameter sets
         \item
            prefers LArSoft data products as input and output
         \item
            elaborates a single event or part of an event at a time
      \end{itemize}
   \item
      a module for the \ART framework, that:
      \begin{itemize}
         \item \label{response:0202.1}
           owns and manages the lifetime of one or more algorithm classes
         \item
           provides the algorithm(s) with the configuration, the data products
           and the information it needs to operate
         \item
           delivers algorithm output to the \ART framework
      \end{itemize}
\end{enumerate}
\label{response:0203.1}
LArSoft provides data products for many of the common concepts.
Algorithms that deal with these concepts should consume and produce such shared data products,
pairing them with additional, specific data structures
when more information needs to be carried around.

Since algorithms often rely on services, the services also need to
follow the same factorization model and be split in:

\begin{enumerate}
   \item
      a \emph{service provider}, in the form of a class that:
      \begin{itemize}
         \item
            is configurable with FHiCL parameter sets
         \item
            has the minimal convenient set of dependencies
         \item
            provides actual functionality
      \end{itemize}
   \item
      a service for the \ART framework, that:
      \begin{itemize}
         \item
            owns and manages the lifetime of its service provider
         \item
            provides modules with a pointer to the provider
         \item
            when relevant, reacts to messages from the framework (e.g., the
            beginning of a new run) and propagates them to the provider as needed
      \end{itemize}
\end{enumerate}
The module is also responsible of communicating to its algorithms which
service providers to use.
\label{response:0201.1}
The typical implementation of this pattern in the \ART framework is the following:
the module, when setting up the algorithm (possibly on each event),
obtains the services it needs from the framework,
obtains the provider from each of these services,
and uses the algorithm interface to propagate these providers to the algorithm itself.
\label{response:0205.1}
Algorithms exclusively interact with service providers rather than with \ART services.

Other important guidelines for the development of algorithms are:

\begin{description}
   \item[interoperability]
      they should document their assumptions in detail,
      and correctly perform on any detector if possible
   \item[modularity]
      each algorithm should perform a single task;
      complex tasks can be performed by hierarchies of algorithms
   \item[maintainability]
      they should come with complete documentation and proper tests
\end{description}

\Cref{fig:AlgorithmModel} shows that if algorithms are not
framework-dependent, their unit test can also be framework-independent.
Therefore, not only those algorithms can be developed in a simplified,
framework-unaware environment, but they can also be tested in that same
development environment. In other words, the full development cycle, of
which testing is an integral part, can seamlessly happen in the same
environment.


\subsection{External libraries}
\label{ssec:Development:ExternalLibraries}

We call ``external'' any library that does not depend on LArSoft, with
the possible exception of its data products. Examples in this category
are \GENIE, \GEANT, and \Pandora.

LArSoft's modularity can accommodate contributions from external
libraries into its workflow (\cref{fig:LArSoftAndExternals}). The
preferred way is to use directly the external library via its interface.
This requires an additional interface module between LArSoft and the
library, in charge of converting the LArSoft data products into a format
digestible by the external library, configuring and driving it, and
extracting and converting the results into a set of LArSoft data
products.
\begin{figure}
   \centering
   \includegraphics{figures/LArSoftAndExternalLibrary}
   \caption[Interaction between LArSoft and an external library]{
      \label{fig:LArSoftAndExternals}
      Interaction between LArSoft and an external library.
      The dashed line encompasses the components belonging to LArSoft.
      Shapes and colours are as in \cref{fig:LArSoftSimulationRelations}.
   }
\end{figure}

This model is exemplified in the interaction between LArSoft and
\Pandora\cite{pandora} (\cref{fig:LArSoftAndPandora}): \Pandora uses
its own data classes for input hits, particle flow results and geometry
specification. A base module exists that reads LArSoft hits, converts
them into \Pandora's, translates geometry information,
and LArSoft clusters, tracks, vertices, and more,
out of \Pandora particle flow objects.
\begin{figure}
   \centering
   \includegraphics{figures/LArSoftAndPandora}
   \caption[Model of communication between LArSoft and \Pandora]{
      \label{fig:LArSoftAndPandora}
      LArSoft workflow including \Pandora.
      Data structures in the \texttt{pandora} namespace are defined in \Pandora
      and also known by LArSoft.
   }
\end{figure}\\
This approach has relevant advantages: it can be fairly fast; it allows
a precise translation of information; it provides the greatest control
on the flow within the library; it defines and tracks the configuration
of the external library. Its greatest drawback is the need for the
LArSoft interface to depend on the external library. If this limitation
is not acceptable, a more independent communication channel can be
established via exchange files. In this case, LArSoft interface
translates data products into a neutral format, possibly based solely on
ROOT objects or on a textual representation, and back into data
products. The external library is in charge of performing the equivalent
operations with the library data format. This is for example the generic
communication mechanism with event generators that support HEPEVT
format. The strong decoupling comes at the price of a fragmented
execution chain and the burden of additional configuration consistency
control, for example to ensure that a consistent geometry was used for
the information (re)entering LArSoft.
